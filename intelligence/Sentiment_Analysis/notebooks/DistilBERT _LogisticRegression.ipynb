{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f39b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# NLTK (ensure you have downloaded stopwords)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers / Torch (used only for DistilBERT)\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d63a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1806db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"G:\\Bank_Bot\\Sentiment_Analysis\\data\\data.csv\"  \n",
    "OUTPUT_DIR = r\"G:\\Bank_Bot\\Sentiment_Analysis\\model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_SIZE = 10000   # for quicker runs - increase for better performance\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "# For DistilBERT training - tune these\n",
    "BATCH_SIZE = 16 if USE_GPU else 8\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4e4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74cc8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # remove URLs, mentions, hashtags, HTML, and non-letters (keep spaces)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = text.lower().strip()\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in text.split() if w not in STOP_WORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a8ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (5841, 3)\n",
      "                                                text     label  \\\n",
      "0  The GeoSolutions technology will leverage Bene...  positive   \n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative   \n",
      "2  For the last quarter of 2010 , Componenta 's n...  positive   \n",
      "3  According to the Finnish-Russian Chamber of Co...   neutral   \n",
      "4  The Swedish buyout firm has sold its remaining...   neutral   \n",
      "\n",
      "                                          clean_text  \n",
      "0  geosolutions technology leverage benefon gps s...  \n",
      "1                       esi lows bk real possibility  \n",
      "2  last quarter componenta net sales doubled eur ...  \n",
      "3  according finnish russian chamber commerce maj...  \n",
      "4  swedish buyout firm sold remaining percent sta...  \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if not {\"Sentence\", \"Sentiment\"}.issubset(df.columns):\n",
    "    raise RuntimeError(\"CSV must contain 'Sentence' and 'Sentiment' columns.\")\n",
    "\n",
    "df = df.rename(columns={\"Sentence\": \"text\", \"Sentiment\": \"label\"})\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(basic_clean)\n",
    "\n",
    "# Drop missing\n",
    "df = df.dropna(subset=[\"text\", \"label\"])\n",
    "df = df[df[\"clean_text\"].str.strip() != \"\"]\n",
    "\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a11365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"].astype(str))\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.joblib\"))\n",
    "print(\"Label classes:\", list(label_encoder.classes_))\n",
    "\n",
    "# Optional sampling for quick testing\n",
    "if SAMPLE_SIZE and SAMPLE_SIZE < len(df):\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0b7246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Val sizes: 4672 1169\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"label\"].tolist()\n",
    ")\n",
    "print(\"Train / Val sizes:\", len(train_texts), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b64056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "LR Accuracy: 0.6638152266894782\n",
      "LR Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.43      0.37       172\n",
      "           1       0.76      0.73      0.74       626\n",
      "           2       0.72      0.67      0.69       371\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.60      0.61      0.60      1169\n",
      "weighted avg       0.68      0.66      0.67      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "X_train = tfidf.fit_transform(train_texts)\n",
    "X_val = tfidf.transform(val_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model.fit(X_train, train_labels)\n",
    "\n",
    "val_preds_lr = lr_model.predict(X_val)\n",
    "print(\"LR Accuracy:\", accuracy_score(val_labels, val_preds_lr))\n",
    "print(\"LR Classification Report:\\n\", classification_report(val_labels, val_preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d2a557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TF-IDF vectorizer and LogisticRegression to G:\\Bank_Bot\\Sentiment_Analysis\\model\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(tfidf, os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "joblib.dump(lr_model, os.path.join(OUTPUT_DIR, \"logreg_tfidf.joblib\"))\n",
    "print(\"Saved TF-IDF vectorizer and LogisticRegression to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91946adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train/val counts: 4672 1169\n"
     ]
    }
   ],
   "source": [
    "raw_texts = df[\"text\"].astype(str).tolist()\n",
    "raw_labels = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "train_texts_raw, val_texts_raw, train_labels_raw, val_labels_raw = train_test_split(\n",
    "    raw_texts, raw_labels, test_size=0.2, random_state=RANDOM_STATE, stratify=raw_labels\n",
    ")\n",
    "\n",
    "print(\"Raw train/val counts:\", len(train_texts_raw), len(val_texts_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a063aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared DistilBERT datasets: 4672 1169\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(tokenizer_name)\n",
    "\n",
    "class HF_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len)\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = HF_Dataset(train_texts_raw, train_labels_raw, tokenizer, max_len=MAX_LEN)\n",
    "val_dataset = HF_Dataset(val_texts_raw, val_labels_raw, tokenizer, max_len=MAX_LEN)\n",
    "print(\"Prepared DistilBERT datasets:\", len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d3aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer created ✅ — Run `trainer.train()` to fine-tune DistilBERT.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(set(raw_labels))\n",
    "model = DistilBertForSequenceClassification.from_pretrained(tokenizer_name, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"distilbert_sentiment\"),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=LR,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer, \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer created ✅ — Run `trainer.train()` to fine-tune DistilBERT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1643c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_lr(texts: List[str]):\n",
    "    cleaned = [basic_clean(t) for t in texts]\n",
    "    X = tfidf.transform(cleaned)\n",
    "    preds = lr_model.predict(X)\n",
    "    probs = lr_model.predict_proba(X) if hasattr(lr_model, \"predict_proba\") else None\n",
    "    return preds.tolist(), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b53f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_distilbert(texts: List[str], model_obj=None, tokenizer_obj=None, device=None):\n",
    "    if model_obj is None:\n",
    "        model_obj = model\n",
    "    if tokenizer_obj is None:\n",
    "        tokenizer_obj = tokenizer\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_obj.to(device)\n",
    "    enc = tokenizer_obj(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_obj(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "    return preds, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3dba45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text: str, backend=\"bert\"):\n",
    "    if backend == \"lr\":\n",
    "        pred, probs = predict_with_lr([text])\n",
    "        label = int(pred[0])\n",
    "        label_name = label_encoder.inverse_transform([label])[0]\n",
    "        return {\"label\": label_name, \"probabilities\": probs.tolist() if probs is not None else None}\n",
    "    else:\n",
    "        preds, logits = predict_with_distilbert([text])\n",
    "        label = int(preds[0])\n",
    "        label_name = label_encoder.inverse_transform([label])[0]\n",
    "        return {\"label\": label_name, \"logits\": logits.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df281cdf-7e11-4d5c-9683-bad791688945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1168' max='1168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1168/1168 1:35:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1168, training_loss=0.29197188762769305, metrics={'train_runtime': 5709.7692, 'train_samples_per_second': 1.636, 'train_steps_per_second': 0.205, 'total_flos': 158036424901632.0, 'train_loss': 0.29197188762769305, 'epoch': 2.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541e22c-1149-4f3a-9813-3bc0b1da29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0c3b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"I love the banking app interface!\",\n",
    "    \"My payment failed twice and I am frustrated.\",\n",
    "    \"The customer support was very polite and helpful.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c56544d-9d70-400b-ba35-555b639be806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Predictions (fine-tuned): [2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT Predictions (fine-tuned):\", predict_with_distilbert(examples)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dc077f6-8abc-486a-bb7d-e93a909ae014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "preds = predict_with_distilbert(examples)[0]\n",
    "print([label_map[p] for p in preds])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
